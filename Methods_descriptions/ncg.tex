
It is assumed, that each component of labelling $z$ has a polynomial distribution with parameter $\pi$
and each element of $A$ has a Bernoulli distribution with parameter $P_{z(i)z(j)}$:

\begin{equation}
    \begin{aligned}
    \nonumber
        & z_i \sim \text{Poly}(\pi), \quad \pi^T = (\pi_{1}, \dots, \pi_{k}), \\
        & a_{ij} \sim \text{Bernoulli}(P_{z(i)z(j)}), \quad i,j = \overline{1, n}, \\
    \end{aligned}
\end{equation}
A Bayesian approach is used to estimate the most probable configuration of $z$ given an adjacency matrix $A$

\begin{equation}
    \begin{aligned}
    \label{maxz}
    z^* = \arg\max\limits_z p(z | A) = \arg\max\limits_z \iint p(z, \pi, P | A) d\pi dP
    \end{aligned}
\end{equation}
It treats parameters $\pi$ and $\theta_{ij}$ as random variables with following prior distributions

\begin{equation}
    \begin{aligned}
    \nonumber
        & \pi \sim \text{Dirichlet}(\alpha) \\
        & P_{ii} \sim \text{Beta}(\beta), \quad i = \overline{1,k}, \\
    \end{aligned}
\end{equation}
where $\alpha$ and $\beta$ are predefined hyperparameters
and $P_{ij}$, $i\neq j$ are set to equal to a small constant $\varepsilon$.
Futhermore, to tackle the intractable integration in \ref{maxz} a restriction on the family of factorized distributions is considered

\begin{equation}
    \begin{aligned}
    \nonumber
    \mathcal Q = \{q:\, q(z, \pi, P) = q(\pi)q(P) \prod\limits_i q(z_i) \},
    \end{aligned}
\end{equation}
where
\begin{equation}
    \begin{aligned}
    \nonumber
    & q(z_i): z_i \sim \text{Poly}(\tilde \pi) \\
    & q(\pi): \pi \sim \text{Dirichlet}(\tilde \alpha) \\
    & q(P): P_{ii} \sim \text{Beta}(\tilde\beta_i)
    \end{aligned}
\end{equation}

The optimal distribution $q^* \in \mathcal Q$, that approximates the true posterior $p(z, \pi, P | A)$ minimizes the Kullback-Leibler divergence

\begin{equation}
    \begin{aligned}
    \label{minq}
    q^* = \arg\min\limits_{q \in \mathcal Q} \text{KL} \left( q \| p(z, \pi, P | X) \right)
    \end{aligned}
\end{equation}

Define
\begin{equation}
    \begin{aligned}
    \mathcal L(q) = \sum\limits_z \iint q(z, \pi, P) \log \frac{p(A, z, \pi, P)}{q(z, \pi, P)} d\pi dP
    \end{aligned}
\end{equation}
Since

\begin{equation}
    \begin{aligned}
    \nonumber
    \mathcal L(q) + \text{KL} \left( q \| p(z, \pi, P | X) \right) = \log p(A)
    \end{aligned}
\end{equation}
the minimization problem \ref{minq} can be equivalently solved by maximizing $\mathcal L(q)$

\begin{equation}
    \begin{aligned}
    \label{maxq}
    q^* = \arg\max\limits_{q \in \mathcal Q} \mathcal L(q)
    \end{aligned}
\end{equation}

Denote $\tilde\Pi = \| \tilde\pi_{ij} \|$, $i = \overline{1, n}$, $j = \overline{1, k}$.
Given $\tilde\Pi$ values of parameters, that maximize $\mathcal L(q)$ can be found according to formulas

\begin{equation}
    \begin{aligned}
    \label{alpha_beta}
    & \tilde\alpha = \alpha + \tilde\Pi^T 1 \\
    & \tilde \beta_i = \beta + \frac12 \left( \tilde\Pi_{\cdot i}^T A \tilde\Pi_{\cdot i}, \tilde\Pi_{\cdot i}^T \overline A \tilde\Pi_{\cdot i} \right)^T, \quad i = \overline{1, k},
    \end{aligned}
\end{equation}
where $1$ denotes an all-ones vector, $\overline A = 11^T - I - A$, and $\tilde\Pi_{\cdot i}$ stands for the $i$\/-th column of the matrix $\tilde\Pi$.
A corresponding value of log-likelihood is equal to
\begin{equation}
    \begin{aligned}
    \label{L}
    \mathcal L(\tilde\Pi) = \sum\limits_{i < j}  \tilde\Pi_{\cdot i}^T A \tilde\Pi_{\cdot j} \log\varepsilon + \tilde\Pi_{\cdot i}^T \overline A \tilde\Pi_{\cdot j}\log(1 - \varepsilon) - \sum\limits_{i,j} \tilde\pi_{ij} \log\tilde\pi_{ij} + \log\frac{\mathcal B(\tilde\alpha)}{\mathcal B(\alpha)} + \sum\limits_i \log\frac{\mathcal B(\tilde\beta)}{\mathcal B(\beta)},
    \end{aligned}
\end{equation}
where $\tilde\alpha = \tilde\alpha(\tilde\Pi)$ and $\tilde\beta = \tilde\beta(\tilde\Pi)$ can be found from \ref{alpha_beta}, $\mathcal B(\alpha) \triangleq \frac{\Gamma\left( \sum\limits_i \alpha_i \right)}{\prod\limits_i \Gamma(\alpha_i)}$ and $\Gamma$ is gamma-function.
Now the maximization problem \ref{maxq} can be reformulated as follows
\begin{equation}
    \begin{aligned}
    \begin{cases}
        \mathcal L(\tilde\Pi) \longrightarrow \max \\
        \sum\limits_{j} \tilde\pi_{ij} = 1, \quad i = \overline{1,n}
    \end{cases}
    \end{aligned}
\end{equation}
One can use reparametrization

\begin{equation}
    \begin{aligned}
    \label{pi}
    & \tilde\pi_{ij} = e^{\theta_{ij} - \mathcal A_i}, \quad i = \overline{1, n}, j = \overline{1, k-1}, \\
    & \tilde\pi_{ik} = e^{-\mathcal A_i}, \quad i = \overline{1, n}, \\
    & \mathcal A_i = \log \left( 1 + \sum\limits_{j=1}^{k-1} e^{\theta_{ij}} \right), \quad \quad i = \overline{1, n} \\
    \end{aligned}
\end{equation}
and obtain a problem of unconstrained maximization

\begin{equation}
    \begin{aligned}
    \label{max_theta}
    \mathcal L(\theta) \longrightarrow \max
    \end{aligned}
\end{equation}

The problem \ref{max_theta} can be solved via natural conjugate gradient method.
Namely, given an initial value $\theta^{(0)}$, one iteratively finds optimal value of $\theta$ as follows
\begin{equation}
    \begin{aligned}
    \nonumber
    \theta^{(t+1)} = \theta^{(t)} + \lambda^{(t)} d^{(t)}, \quad t = 0, 1, 2, \dots
    \end{aligned}
\end{equation}
Here $d^{(t)}$ is so called natural conjugate gradient.
$d^{(t)}$ can be found according to formulas

\begin{equation}
    \begin{aligned}
    \label{d}
    d^{(t)} =
    \begin{cases}
        g^{(t)}, \quad t = 0 \\
        g^{(t)} + \frac{\| g^{(t)} \|_\theta^2}{\| g^{(t-1)} \|_\theta^2} d^{(t-1)}, \quad t > 0,
    \end{cases}
    \end{aligned}
\end{equation}
where $g^{(t)}$ is a natural gradient of $\mathcal L(\theta)$.
$\| \cdot \|_\theta$ stands for the norm with respect to Riemannian metrics
\begin{equation}
    \begin{aligned}
    \nonumber
    G(\theta) = \text{diag} \left( \mathcal I(\theta_1), \dots, \mathcal I(\theta_N) \right),
    \end{aligned}
\end{equation}
where $\theta_i = (\theta_{i1}, \dots, \theta_{i,k-1})^T$, $i = \overline{1, n}$ and $\mathcal I(\theta_i)$ is a Fischer information.
This method is nothing else but a conjugate gradient method in a Riemannian space with metrics $G(\theta)$.
Given $\theta$, $g$ and $\|g\|_\theta$ can be found as follows
\begin{equation}
    \begin{aligned}
    \label{ng}
    & g = \nabla_{\tilde\Pi} \mathcal L(\tilde\Pi) =
    \begin{pmatrix}
        (I, -1) \nabla_{\tilde\pi_1} \mathcal L(\tilde\Pi) \\
        \vdots \\
        (I, -1) \nabla_{\tilde\pi_n} \mathcal L(\tilde\Pi)
    \end{pmatrix} \\
    & \| g \|_\theta^2 = \sum\limits_{i = 1}^n \left( \nabla_{\tilde\pi_1} \mathcal L(\tilde\Pi) \right)^T \left( \text{diag}(\tilde\pi_i) - \tilde\pi_i^T \tilde\pi_i \right) \left( \nabla_{\tilde\pi_1} \mathcal L(\tilde\Pi) \right),
    \end{aligned}
\end{equation}
where $\tilde\Pi = \tilde\Pi(\theta)$
and
\begin{multline}
    \label{L_prime}
    \frac{\partial \mathcal L(\tilde\Pi)}{\partial \pi_{ij}} = \sum\limits_{l \neq j} \left( A_{i\cdot}\tilde\Pi_{\cdot l} \log\varepsilon + \overline A_{i\cdot}\tilde\Pi_{\cdot j} \log(1 - \varepsilon) \right) + \\
    \left( A_{i\cdot}\tilde\Pi_{\cdot j}, \overline A_{i\cdot}\tilde\Pi_{\cdot j} \right) \nabla\log\mathcal B (\tilde\beta_{j}) + \psi(\tilde\alpha_j) - \log\tilde\pi_{ij} - 1,
\end{multline}
where $\psi(\cdot)$ is digamma function.

The final algorithm is given in \ref{ncg_alg}.

\begin{algorithm}[H]
    \caption{Natural Conjugate Gradient}
    \label{ncg_alg}
	\SetAlgoLined

	\KwIn{adjacency matrix $A$, maximum number of clusters $k$, tolerance $\eta$, maximum number of iterations $t_{\max}$}
	\KwOut{array of predicted labels Z}
	Initialize $\theta$; \\
    $\mathcal L_{old} \leftarrow -\infty$; \\
    $\lambda \leftarrow 1$; \\

    \For{$t$ in range ($t_{\max}$)}
    {
        Calculate $\tilde\Pi$ using \ref{pi}; \\
        Calculate $\tilde\alpha$, $\tilde\beta$ using \ref{alpha_beta}; \\
        Calculate $\mathcal L$ using \ref{L}; \\

        \uIf{
                $0 \leq \frac{\mathcal L - \mathcal L_{old}}{|\mathcal L|} < \eta$
            }
            {
                \bf break
            }
        \eIf{
                $\eta \geq 0$
            }
            {
                Update $d$ using \ref{d}, \ref{ng}, \ref{L_prime}; \\
                $\theta_{old} \leftarrow \theta$ \\
                $\theta \leftarrow \theta_{old} + \lambda d$; \\
                $\mathcal L_{old} \leftarrow \mathcal L$; \\
            }
            {
                $\lambda \leftarrow \frac\lambda2$ \\
                $\theta \leftarrow \theta_{old} + \lambda |\eta| d$ \\
            }
    }

    $Z = \left( \arg\max\limits_{1\leq j \leq k} \tilde\pi_{1j}, \dots, \arg\max\limits_{1\leq j \leq k} \tilde\pi_{nj} \right)$

    \Return $Z$	
\end{algorithm}

Initial value of $\theta$ was generated from a standard normal distribution $\mathcal N(0, 1)$.
Probability of occurrence an inter-cluster edge $\varepsilon$ was set to $10^{-10}$.
The maximal number of iterations and relative tolerance were taken equal to $100$ and $10^{-6}$ respectively.
Examples of performance of the natural conjugate gradient method can be found, for example, in \cite{ncg}.