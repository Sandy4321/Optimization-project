{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Idea\n",
    "\n",
    "$\\textbf{Generating graph for arbitrary data }$\n",
    "\n",
    "Given a set of data points $x_1,\\dots, x_n$ and some notion of similarity $s_{ij} \\geq 0$ (example: $s(x_i, x_j ) = \\exp(\\frac{−\\|x_i − x_j \\|^2}{2\\sigma^2})$) between all pairs of data points $x_i$ and $x_j$ , the intuitive goal of clustering is to divide the data points into several groups such that points in the same group are similar and points in different groups are dissimilar to each other. In the corresponding graph there is no edge between pair of vertices is $s_{ij} = 0$ and, otherwise, there is an edge between $i,j$ with weight $s_{ij}$\n",
    "\n",
    "Since we are not constructing a graph from the data, but just simply use given ones, we think of two vertices to be similar ($s_{ij}=1$) if there is an edge between them and zero otherwise.\n",
    "\n",
    "$\\textbf{Normalized and non-normalized Laplacians}$\n",
    "\n",
    "Unnormalized Graph Laplacian (strong connection between connected components in a graph and the spectrum of Laplacian):\n",
    "\n",
    "$$\n",
    "L=D-A (\\text{or} \\ D-W  \\ \\text{in case of weighted graph})\n",
    "$$\n",
    "\n",
    "Normalized Laplacians:\n",
    "\n",
    "\n",
    "$$\n",
    "L_{sym} := D^{−1/2}LD^{−1/2} = I − D^{−1/2}W D^{−1/2}\\\\\n",
    "L_{rw} :=D^{−1}L=I−D^{−1}W.\n",
    "$$\n",
    "\n",
    "$\\textbf{Measuring sizes of subsets}$\n",
    "\n",
    "Let $A_{1},\\dots,A_{k}$ be some subsets of vertices. Two ways of measuring sizes of subsets are considered:\n",
    "\n",
    "$$\n",
    "|A_{i}| = \\{\\text{number of vertices in $A_{i}$}\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "vol(A_{i}) = \\sum \\limits_{i \\in A_{i}}d_{i}\n",
    "$$\n",
    "\n",
    "where $d_{i}$ is the degree of vertex $i$.\n",
    "\n",
    "$\\textbf{Intuition behind spectral clustering algorithms}$\n",
    "\n",
    "For given graph, the problem can be restated the following way: one wants to find a partition of the graph such that the edges between different groups have a very low weight (or there are not that many egdes between different groups) and the edges within a group have high weight (points within the same cluster are similar to each other and, hence, there are a lot of edges within the group). Spectral clustering can be derived as an approximation to such graph partitioning problems. Define for arbitrary sets $A,B$:\n",
    "\n",
    "$$\n",
    "W(A,B):= \\sum \\limits_{i \\in A,j \\in B}  w_{ij}.\n",
    "$$\n",
    "\n",
    "Hence, the simpliest way to get partition - solve mincut problem ($\\overline{A}$ denotes complement of $A$):\n",
    "\n",
    "$$\n",
    "cut(A_1,\\dots,A_k) = \\frac{1}{2}\\sum\\limits_{i=1}^kW(A_i,\\overline{A_i}) \\rightarrow \\min\n",
    "$$\n",
    "\n",
    "But in many cases the solution of mincut simply separates one individual vertex from the rest of the graph. To overcome such problems (to balance subsets) the following values are considered:\n",
    "\n",
    "$$\n",
    "RatioCut(A_1,\\dots,A_k) = \\sum \\limits_{i=1}^k \\frac{cut(A_i,\\overline{A_i})}{|A_{i}|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Ncut(A_1,\\dots,A_k)= \\sum \\limits_{i=1}^k \\frac{cut(A_i,\\overline{A_i})}{vol(A_{i})}\n",
    "$$\n",
    "\n",
    "Unfortunately, introducing balancing conditions makes the previously simple to solve mincut problem become NP hard. Spectral clustering is a way to solve relaxed versions of those problems (relaxing RatioCut provides unnormalized spectral clustering, relaxing Ncut provides normalized spectral clustering with using $L_{rw}$).\n",
    "\n",
    "Also spectral clustering can be interpreted as trying to find a partition of the graph such that the random walk stays long within the same cluster and seldom jumps between clusters (there is a connection with Ncut problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Lib.Load import download_graph, download_labels, save_graph, save_labels\n",
    "from Lib.Metrics import compute_nmi, compute_recall, compute_precision, compute_avg_f1, compute_modularity, compute_normalized_cut\n",
    "from Lib.Transformations import compute_clusters_from_labels, compute_adj_mat\n",
    "from Lib.Algorithms import get_degree, get_nonnorm_lapl, get_lapl_sym,get_lapl_rw,unnorm_predict,lrw_predict,lsym_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnormalized spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm is the following (considering case when there are $k$ clusters and also when talking about eigenvalues consider the smallest ones):\n",
    "\n",
    "$\\cdot$ Given adjacency matrix $A$, compute its Laplacian $L$\n",
    "\n",
    "$\\cdot$ Compute the first $k$ eigenvectors $u_1, \\dots , u_k$ of $L$. Let $U \\in \\mathbb{R}^{n\\times k}$ be the matrix containing the vectors $u_1 , \\dots, u_k$ as columns.\n",
    "\n",
    "$\\cdot$ For $i = 1,\\dots,n$, let $y_i \\in \\mathbb{R}^k$ be the vector or the $i$-th row of $U$\n",
    "\n",
    "$\\cdot$ Cluster the points $(y_i),i=1,\\dots,n$ with the $k$-means algorithm into clusters $C_1,...,C_k$\n",
    "\n",
    "Now we simply check on our graphs\n",
    "\n",
    "$\\textbf{SBM}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  1.0\n",
      "Recall =  1.0\n",
      "Precision =  1.0\n",
      "F1 =  1.0\n",
      "Modularity =  0.249741735537\n",
      "Norm. cut =  0.500258397933\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('sbm')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('sbm')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = unnorm_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Karate}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.455001600015\n",
      "Recall =  0.8125\n",
      "Precision =  0.875\n",
      "F1 =  0.813186813187\n",
      "Modularity =  0.233974358974\n",
      "Norm. cut =  0.376068376068\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('karate')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('karate')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = unnorm_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Football}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.92419578687\n",
      "Recall =  0.889285714286\n",
      "Precision =  0.873677248677\n",
      "F1 =  0.886670290398\n",
      "Modularity =  0.600516540747\n",
      "Norm. cut =  4.04619502004\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('football')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('football')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = unnorm_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Polbooks}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.651179091301\n",
      "Recall =  0.722232363428\n",
      "Precision =  0.839196310935\n",
      "F1 =  0.733535851679\n",
      "Modularity =  0.450527300867\n",
      "Norm. cut =  0.616538296962\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('polbooks')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('polbooks')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = unnorm_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized spectral clustering with $L_{rw}$\n",
    "\n",
    "Algorithm is the following (considering case when there are $k$ clusters and also when talking about eigenvalues consider the smallest ones):\n",
    "\n",
    "$\\cdot$ Given adjacency matrix $A$, compute its normalized Laplacian $L_{rw}$\n",
    "\n",
    "$\\cdot$ Compute the first $k$ eigenvectors $u_1, \\dots , u_k$ of $L_{rw}$. Let $U \\in \\mathbb{R}^{n\\times k}$ be the matrix containing the vectors $u_1 , \\dots, u_k$ as columns.\n",
    "\n",
    "$\\cdot$ For $i = 1,\\dots,n$, let $y_i \\in \\mathbb{R}^k$ be the vector or the $i$-th row of $U$\n",
    "\n",
    "$\\cdot$ Cluster the points $(y_i),i=1,\\dots,n$ with the $k$-means algorithm into clusters $C_1,...,C_k$\n",
    "\n",
    "Now we simply check on our graphs\n",
    "\n",
    "$\\textbf{SBM}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  1.0\n",
      "Recall =  1.0\n",
      "Precision =  1.0\n",
      "F1 =  1.0\n",
      "Modularity =  0.249741735537\n",
      "Norm. cut =  0.500258397933\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('sbm')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('sbm')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lrw_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Karate}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.313617263711\n",
      "Recall =  0.71875\n",
      "Precision =  0.833333333333\n",
      "F1 =  0.704347826087\n",
      "Modularity =  0.192554240631\n",
      "Norm. cut =  0.422764227642\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('karate')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('karate')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lrw_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Football}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.92419578687\n",
      "Recall =  0.889285714286\n",
      "Precision =  0.873677248677\n",
      "F1 =  0.886670290398\n",
      "Modularity =  0.600516540747\n",
      "Norm. cut =  4.04619502004\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('football')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('football')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lrw_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Polbooks}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.623260331179\n",
      "Recall =  0.724617575116\n",
      "Precision =  0.750069803728\n",
      "F1 =  0.732017982018\n",
      "Modularity =  0.48448691646\n",
      "Norm. cut =  0.414272054397\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('polbooks')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('polbooks')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lrw_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized spectral clustering with $L_{sym}$\n",
    "\n",
    "Algorithm is the following (considering case when there are $k$ clusters and also when talking about eigenvalues consider the smallest ones):\n",
    "\n",
    "$\\cdot$ Given adjacency matrix $A$, compute its normalized Laplacian $L_{sym}$\n",
    "\n",
    "$\\cdot$ Compute the first $k$ eigenvectors $u_1, \\dots , u_k$ of $L_{sym}$. Let $U \\in \\mathbb{R}^{n\\times k}$ be the matrix containing the vectors $u_1 , \\dots, u_k$ as columns.\n",
    "\n",
    "$\\cdot$ Form the matrix $T \\in \\mathbb{R}^{n\\times k}$ from $U$ by normalizing the rows to norm 1\n",
    "\n",
    "$\\cdot$ For $i = 1,\\dots,n$, let $t_i \\in \\mathbb{R}^k$ be the vector or the $i$-th row of $T$\n",
    "\n",
    "$\\cdot$ Cluster the points $(t_i),i=1,\\dots,n$ with the $k$-means algorithm into clusters $C_1,...,C_k$\n",
    "\n",
    "Now we simply check on our graphs\n",
    "\n",
    "$\\textbf{SBM}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  1.0\n",
      "Recall =  1.0\n",
      "Precision =  1.0\n",
      "F1 =  1.0\n",
      "Modularity =  0.249741735537\n",
      "Norm. cut =  0.500258397933\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('sbm')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('sbm')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lsym_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Karate}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.836504088906\n",
      "Recall =  0.96875\n",
      "Precision =  0.973684210526\n",
      "F1 =  0.970357454228\n",
      "Modularity =  0.359960552268\n",
      "Norm. cut =  0.262626262626\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('karate')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('karate')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lsym_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Football}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.92419578687\n",
      "Recall =  0.889285714286\n",
      "Precision =  0.873677248677\n",
      "F1 =  0.886670290398\n",
      "Modularity =  0.600516540747\n",
      "Norm. cut =  4.04619502004\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('football')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('football')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lsym_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Polbooks}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI =  0.542276479422\n",
      "Recall =  0.74093680406\n",
      "Precision =  0.732554200542\n",
      "F1 =  0.735050982118\n",
      "Modularity =  0.501504002962\n",
      "Norm. cut =  0.481903537479\n"
     ]
    }
   ],
   "source": [
    "n_nodes, edge_list = download_graph('polbooks')\n",
    "A = compute_adj_mat(n_nodes, edge_list)\n",
    "\n",
    "labels_true = download_labels('polbooks')\n",
    "clusters_true = compute_clusters_from_labels(labels_true)\n",
    "\n",
    "num_of_clusters = np.max(labels_true) + 1\n",
    "\n",
    "labels_pred = lsym_predict(A, num_of_clusters)\n",
    "clusters_pred = compute_clusters_from_labels(labels_pred)\n",
    "\n",
    "print \"NMI = \", compute_nmi(labels_true, labels_pred)\n",
    "print \"Recall = \", compute_recall(clusters_true, clusters_pred)\n",
    "print \"Precision = \", compute_precision(clusters_true, clusters_pred)\n",
    "print \"F1 = \", compute_avg_f1(clusters_true, clusters_pred)\n",
    "print \"Modularity = \", compute_modularity(labels_pred, edge_list)\n",
    "print \"Norm. cut = \", compute_normalized_cut(labels_pred, clusters_pred, edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
